# error, CTL
CTL_err_coef = lmer(conf ~ logRT + (1 + logRT|subj), data=CTLData_err
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(CTL_err_coef)
fix.se <- sqrt(diag(vcov(CTL_err_coef)))
betas <- c(fix, fix.se)
write.csv(betas, file = paste(asdDir, 'regression_betas_CTL_err.csv'))
## get the coefficients for Figure 3a
# lowMENTA
lmenta_coef = lmer(conf ~ logRT + (1 + logRT|subj), data=bigData_lmenta
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(lmenta_coef)
fix.se <- sqrt(diag(vcov(lmenta_coef)))
betas <- c(fix, fix.se)
write.csv(betas, file = paste(asdDir, 'regression_betas_lmenta.csv'))
# highMENTA
hmenta_coef = lmer(conf ~ logRT + (1 +  logRT|subj), data=bigData_hmenta
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(hmenta_coef)
fix.se <- sqrt(diag(vcov(hmenta_coef)))
betas <- c(fix, fix.se)
write.csv(betas, file = paste(asdDir, 'regression_betas_hmenta.csv'))
library(Rmisc) ##for summ stats
require(lme4)
require(car)
library(ggplot2)
library(nlme)
library(plyr)
library(outliers)
library(reshape2)##to rearrange data for plot autho
library(R.matlab)
## set the path & load in data
dataDir <- "~/Dropbox/Populism_PoliticalAttitudes/Online_paper/data/"
data <- read.csv(paste(dataDir,"OnlineData.csv", sep = ""), header=TRUE, sep=",", na.strings = c("#NULL!"))
#Facors
data$id <- factor(data$id)
data$Topic<-factor(data$video_2, levels = c(2,1,3), labels = c("climate change","immigration","healthcare"))
data$Appraisal<-factor(data$video_3, levels = c(3, 2,1,3), labels = c("uncertain", "blame","threat"))
## PART 0: subject characteristics
mean(data$leftright, na.rm = T)
sd(data$leftright, na.rm = T)/sqrt(length(data$leftright))
mean(data$INDEX_CLIMATE, na.rm = T)
sd(data$INDEX_CLIMATE, na.rm = T)/sqrt(length(data$INDEX_CLIMATE))
mean(data$INDEX_MIGRATION, na.rm = T)
sd(data$INDEX_MIGRATION, na.rm = T)/sqrt(length(data$INDEX_MIGRATION))
mean(data$INDEX_HEALTH, na.rm = T)
sd(data$INDEX_HEALTH, na.rm = T)/sqrt(length(data$INDEX_HEALTH))
## Now I standardize the covariates
data$age <- scale(data$lft4)
data$sex <- data$geslacht-1.5
data$edu <- scale(data$opleiding)
data$pol_autho <- scale(data$INDEX_AUTHO)
data$pol_scep <- scale(data$INDEX_CYNICISM)
data$PositionCl <- scale(data$INDEX_CLIMATE)
data$PositionHe <- scale(data$INDEX_HEALTH)
data$PositionImm <- scale(data$INDEX_MIGRATION)
data$PVV_pre <- scale(data$PVV_pre)
data$GL_pre <- scale(data$GL_pre)
data$SP_pre <- scale(data$SP_pre)
data$leftright <- scale(data$leftright)
## Now I standardize the DV
data$FEAR <- scale(data$FEAR)
data$ANGER <- scale(data$ANGER)
data$NEG_AFFECT <- scale(data$NEG_AFFECT)
data$PVV_post <- scale(data$PVV_post)
data$GL_post <- scale(data$GL_post)
data$SP_post <- scale(data$SP_post)
data$importance_gezondheidsz <- scale(data$V13_1)
data$importance_immigr <- scale(data$V13_2)
data$importance_milieu <- scale(data$V13_3)
data$video_agreement <- scale(data$V10_4)
data$video_realiability <-scale(data$V10_5)
data$video_sharing <- scale(data$V10_6)
## PART 1: VIDEO VALIDATION
## Behavioural effects: Party attitdues
party_appraisal <- lm(PVV_post ~ Appraisal + Topic + NEG_AFFECT + Topic * NEG_AFFECT + age + sex + edu + pol_autho + pol_scep+ PositionImm + PositionCl + PositionHe +Topic*NEG_AFFECT, data=data)
party_appraisal <- lm(GL_post ~ Appraisal + Topic + NEG_AFFECT + Topic * NEG_AFFECT + age + sex + edu + pol_autho + pol_scep+ PositionImm + PositionCl + PositionHe + Topic*NEG_AFFECT, data=data)
print(summary(party_appraisal))
print(Anova(party_appraisal, type = 3))
coef(summary(party_appraisal))
##PART TWO: EMOTION EFFECTS
#H1.1
negative_appraisal <- lm(NEG_AFFECT ~ Appraisal + Topic + age + sex + edu + pol_autho + pol_scep + PositionImm + PositionCl + PositionHe + leftright, data=data)
print(summary(negative_appraisal))
print(Anova(negative_appraisal, type = 3))
coef(summary(negative_appraisal))
library(Rmisc) ##for summ stats
require(lme4)
require(car)
library(ggplot2)
library(nlme)
library(plyr)
library(outliers)
library(reshape2)##to rearrange data for plot autho
library(R.matlab)
## set the path & load in data
dataDir <- "~/Dropbox/Populism_PoliticalAttitudes/Online_paper/data/"
data <- read.csv(paste(dataDir,"OnlineData.csv", sep = ""), header=TRUE, sep=",", na.strings = c("#NULL!"))
#Facors
data$id <- factor(data$id)
data$Topic<-factor(data$video_2, levels = c(2,1,3), labels = c("climate change","immigration","healthcare"))
data$Appraisal<-factor(data$video_3, levels = c(3,2,1), labels = c("uncertain", "blame","threat"))
## PART 0: subject characteristics
mean(data$leftright, na.rm = T)
sd(data$leftright, na.rm = T)/sqrt(length(data$leftright))
mean(data$INDEX_CLIMATE, na.rm = T)
sd(data$INDEX_CLIMATE, na.rm = T)/sqrt(length(data$INDEX_CLIMATE))
mean(data$INDEX_MIGRATION, na.rm = T)
sd(data$INDEX_MIGRATION, na.rm = T)/sqrt(length(data$INDEX_MIGRATION))
mean(data$INDEX_HEALTH, na.rm = T)
sd(data$INDEX_HEALTH, na.rm = T)/sqrt(length(data$INDEX_HEALTH))
## Now I standardize the covariates
data$age <- scale(data$lft4)
data$sex <- data$geslacht-1.5
data$edu <- scale(data$opleiding)
data$pol_autho <- scale(data$INDEX_AUTHO)
data$pol_scep <- scale(data$INDEX_CYNICISM)
data$PositionCl <- scale(data$INDEX_CLIMATE)
data$PositionHe <- scale(data$INDEX_HEALTH)
data$PositionImm <- scale(data$INDEX_MIGRATION)
data$PVV_pre <- scale(data$PVV_pre)
data$GL_pre <- scale(data$GL_pre)
data$SP_pre <- scale(data$SP_pre)
data$leftright <- scale(data$leftright)
## Now I standardize the DV
data$FEAR <- scale(data$FEAR)
data$ANGER <- scale(data$ANGER)
data$NEG_AFFECT <- scale(data$NEG_AFFECT)
data$PVV_post <- scale(data$PVV_post)
data$GL_post <- scale(data$GL_post)
data$SP_post <- scale(data$SP_post)
data$importance_gezondheidsz <- scale(data$V13_1)
data$importance_immigr <- scale(data$V13_2)
data$importance_milieu <- scale(data$V13_3)
data$video_agreement <- scale(data$V10_4)
data$video_realiability <-scale(data$V10_5)
data$video_sharing <- scale(data$V10_6)
## PART 1: VIDEO VALIDATION
## Behavioural effects: Party attitdues
party_appraisal <- lm(PVV_post ~ Appraisal + Topic + NEG_AFFECT + Topic * NEG_AFFECT + age + sex + edu + pol_autho + pol_scep+ PositionImm + PositionCl + PositionHe +Topic*NEG_AFFECT, data=data)
party_appraisal <- lm(GL_post ~ Appraisal + Topic + NEG_AFFECT + Topic * NEG_AFFECT + age + sex + edu + pol_autho + pol_scep+ PositionImm + PositionCl + PositionHe + Topic*NEG_AFFECT, data=data)
print(summary(party_appraisal))
print(Anova(party_appraisal, type = 3))
coef(summary(party_appraisal))
##PART TWO: EMOTION EFFECTS
#H1.1
negative_appraisal <- lm(NEG_AFFECT ~ Appraisal + Topic + age + sex + edu + pol_autho + pol_scep + PositionImm + PositionCl + PositionHe + leftright, data=data)
print(summary(negative_appraisal))
print(Anova(negative_appraisal, type = 3))
coef(summary(negative_appraisal))
fear_appraisal <- lm(FEAR ~ Appraisal + Topic + age + sex + edu + pol_autho + pol_scep + PositionImm + PositionCl + PositionHe + leftright,data=data)
print(summary(fear_appraisal))
print(Anova(fear_appraisal, type = 3))
coef(summary(fear_appraisal))
#H1.3
anger_appraisal <- lm(ANGER ~ Appraisal + Topic + age + sex + edu + pol_autho + pol_scep + PositionImm + PositionCl + PositionHe + leftright, data=data)
print(summary(anger_appraisal))
print(Anova(anger_appraisal, type = 3))
coef(summary(anger_appraisal))
#H1: Agreement
agr_appraisal <- lm(video_agreement ~ Appraisal + NEG_AFFECT + Topic + age + sex + edu + pol_autho + pol_scep + leftright + PositionImm + PositionCl + PositionHe + pol_autho*Appraisal + NEG_AFFECT*Appraisal, data=data)
print(summary(agr_appraisal))
print(Anova(agr_appraisal, type = 3))
coef(summary(agr_appraisal))
## EVDP 2019 elisa.plas.18@ucl.ac.uk
# This code replicates the Results section in chronological order
rm(list=ls())
require(R.matlab)
require(lme4)
require(car)
require(optimx)
require(pwr)
options(contrasts = c("contr.treatment", "contr.poly")) # This is R defaults but set it anyway to be safe
bigData = NULL
demoData = NULL
nat = c('PKU', 'UCL')
j=1
for (d in 1:2) {
if (d == 1) {
dataDir = "~/Dropbox/Github/CulturalMetacognition/DATA/EXP1/PKU_data/PKU_data/"
filePrefix = "Data_sub_"
suffix = "_2"
subjects =c(seq(101,109), seq(111,115), seq(117,141))
} else if (d == 2) {
dataDir = "~/Dropbox/Github/CulturalMetacognition/DATA/EXP1/UCL_data/UCL_data/"
filePrefix = "Data_sub_"
suffix = "_2"
subjects = c(seq(201,204), seq(206, 227), seq(229, 234), seq(236,242))
}
# Extract out data from mat files
for (s in 1:length(subjects)){
setwd(dataDir)
DATA = readMat(paste(filePrefix,subjects[s],suffix,'.mat',sep=""))
dat = DATA$locDATA
precoh = NULL
postcoh = NULL
RT = NULL
conf = NULL
confRT = NULL
response = NULL
dir = NULL
precoh = dat[,,1]$dots.coherence[1,]
postcoh = dat[,,1]$post.coherence[1,]
coh = sort(unique(precoh))
precoh_cat = ifelse(precoh %in% coh[1], -0.5,
ifelse(precoh %in% coh[2], 0, 0.5))
postcoh_cat = ifelse(postcoh %in% coh[1], -0.5,
ifelse(postcoh %in% coh[2], 0, 0.5))
RT = dat[,,1]$reaction.time.button[1,]
conf = dat[,,1]$mouse.response[1,]
confRT = dat[,,1]$reaction.time.mouse[1,]
response = dat[,,1]$button.response[1,] - 1
response[response == 0] = -1
dir = dat[,,1]$dots.direction[1,]/360
dir[dir == 0.5] = -1
accuracy = response == dir
accuracy_dv = accuracy #accuracy when used as dependent variable: 0/1
accuracy[accuracy == 0] <- -1 #accuracy when used as predictor variable: -1/1
logRT = scale(log(RT))
logConfRT = scale(log(confRT))
subj = rep(j, length(accuracy))
country = rep(d, length(accuracy))
subData1 = data.frame("country"=country, "subj"=subj, "dir"=dir, "precoh"=precoh, "postcoh"=postcoh, "precoh_cat"=precoh_cat, "postcoh_cat"=postcoh_cat,"conf"=conf, "logConfRT"=logConfRT, "response"=response, "dir"=dir, "logRT"=logRT, "accuracy"=accuracy, "accuracy_dv" = accuracy_dv)
#add to larger file
bigData = rbind(bigData, subData1)
j=j+1 # total subject counter
}
setwd(paste(dataDir,nat[d], '_betas', sep = ""))
demoData1<- read.csv(paste(nat[d], '_subject_log.csv',sep=""), header = T, sep = ",", na.strings = "EMPTY")
demoData = rbind(demoData, demoData1)
}
# Factors
bigData$subj <- factor(bigData$subj)
bigData$country <- factor(bigData$country, levels = c(1,2), labels = c("PKU","UCL"))
## distinguish between correct/incorrect & PKU/UCL trials
bigData_correct <- bigData[bigData$accuracy == 1, ]
bigData_incorrect <- bigData[bigData$accuracy == -1, ]
bigData_PKU <- bigData[bigData$country == "PKU",]
bigData_UCL <- bigData[bigData$country == "UCL",]
bigData_PKU_correct <- bigData_correct[bigData_correct$country == "PKU",]
bigData_UCL_correct <- bigData_correct[bigData_correct$country == "UCL",]
bigData_PKU_incorrect <- bigData_incorrect[bigData_incorrect$country == "PKU",]
bigData_UCL_incorrect <- bigData_incorrect[bigData_incorrect$country == "UCL",]
## Demographics described at start of Results section (for Supplementary Table 1, see Github_SuppMat/1.1.Demo.R)
PKU = c(demoData[1:length(subjects),])
UCL = c(demoData[length(subjects)+1:nrow(demoData),])
#age
mean(PKU$age, na.rm = T)
sd(PKU$age, na.rm = T)/sqrt(length(subjects))
mean(UCL$age, na.rm = T)
sd(UCL$age, na.rm = T)/sqrt(length(subjects))
t.test(PKU$age, UCL$age, var.equal = T)
#gender
mean(PKU$gender, na.rm = T)
mean(UCL$gender, na.rm = T)
t.test(PKU$gender, UCL$gender, var.equal = T)
#income: remove outlier in UCL
UCL$income[UCL$income == 3000000] <- NA
#relative to PPP difference UK/China
UCL$income = UCL$income/1.71
mean(PKU$income, na.rm = T)
sd(PKU$income, na.rm = T)/sqrt(length(subjects))
mean(UCL$income, na.rm = T)
sd(UCL$income, na.rm = T)/sqrt(length(subjects))
t.test(PKU$income, UCL$income, var.equal = T)
#IQ
mean(PKU$IQ, na.rm = T)
sd(PKU$IQ, na.rm = T)/sqrt(length(subjects))
mean(UCL$IQ, na.rm = T)
sd(UCL$IQ, na.rm = T)/sqrt(length(subjects))
t.test(PKU$IQ, UCL$IQ, var.equal = T)
## FIRST-ORDER PERFORMANCE
accModel = glmer(accuracy_dv ~ country*precoh_cat + (1 + precoh_cat | subj), data=bigData, family="binomial")
print(summary(accModel))
print(Anova(accModel, type =3))
## RT effects
confModel_RT= lmer(conf ~ country* (logRT + accuracy + logRT*accuracy)  + (1 + accuracy + logRT|subj), data=bigData
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE, REML = FALSE)))
fix <- fixef(confModel_RT)
print(summary(confModel_RT))
print(Anova(confModel_RT, type = 3))
coef(summary(confModel_RT)) #get the contrast statistics
## RT effects
confModel_RT= lmer(conf ~ country* (logRT + accuracy + logRT*accuracy)  + (1 + accuracy + logRT|subj), data=bigData
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE, REML = FALSE)))
fix <- fixef(confModel_RT)
print(summary(confModel_RT))
print(Anova(confModel_RT, type = 3))
coef(summary(confModel_RT)) #get the contrast statistics
rm(list=ls())
require(R.matlab)
require(lme4)
require(car)
require(optimx)
bigData = NULL
demoData = NULL
nat = c('PKU', 'UCL')
j=1
for (d in 1:2) {
if (d == 1) {
dataset = "PKU"
dataDir = "~/Dropbox/Github/CulturalMetacognition/DATA/EXP2/PKU_data/PKU_data/"
filePrefix = "Data_sub_"
suffix = "_2"
subjects = c(403, seq(404,418), seq(420,432), seq(434,435), seq(437,443), seq(445,459))
} else if ( d == 2) {
dataset = "UCL"
dataDir = "~/Dropbox/Github/CulturalMetacognition/DATA/EXP2/UCL_data/UCL_data/"
filePrefix = "Data_sub_"
suffix = "_2"
subjects =c(seq(25,76), 79)
}
# Extract out data from mat files
for (s in 1:length(subjects)){
## EXTRACT DATA FILE
setwd(dataDir)
DATA = readMat(paste(filePrefix,subjects[s],suffix,'.mat',sep=""))
dat = DATA$locDATA
precoh = NULL
postcoh = NULL
RT = NULL
conf = NULL
confRT = NULL
response = NULL
dir = NULL
precoh = dat[,,1]$dots.coherence[1,]
postcoh = dat[,,1]$post.coherence[1,]
coh = sort(unique(precoh))
precoh_cat = ifelse(precoh %in% coh[1], -0.5,
ifelse(precoh %in% coh[2], 0, 0.5))
postcoh_cat = ifelse(postcoh %in% coh[1], -0.5,
ifelse(postcoh %in% coh[2], 0, 0.5))
RT = dat[,,1]$reaction.time.button[1,]
conf = dat[,,1]$mouse.response[1,]
confRT = dat[,,1]$reaction.time.mouse[1,]
response = dat[,,1]$button.response[1,] - 1
response[response == 0] = -1
dir = dat[,,1]$dots.direction[1,]/360
dir[dir == 0.5] = -1
accuracy = response == dir
accuracy[accuracy == 0] <- -1
condition = dat[,,1]$condition[1,]
logRT = scale(log(RT))
logConfRT = scale(log(confRT))
subj = rep(j, length(accuracy))
country = rep(d, length(accuracy))
subData1 = data.frame("country"=country, "subj"=subj, "dir"=dir, "precoh"=precoh, "postcoh"=postcoh, "precoh_cat"=precoh_cat, "postcoh_cat"=postcoh_cat,"conf"=conf, "logConfRT"=logConfRT, "response"=response, "dir"=dir, "logRT"=logRT, "accuracy"=accuracy, "condition" = condition)
#add to larger file
bigData = rbind(bigData, subData1)
j=j+1 # total subject counter
}
setwd(paste(dataDir,nat[d], '_betas', sep = ""))
demoData1<- read.csv(paste(nat[d], '_subject_log.csv',sep=""), header = T, sep = ",", na.strings = "EMPTY")
demoData = rbind(demoData, demoData1)
}
# Factors
bigData$subj <- factor(bigData$subj)
bigData$country <- factor(bigData$country, levels = c(1,2), labels = c("PKU","UCL"))
bigData$condition <- factor(bigData$condition, levels = c(1,0), labels = c("social","nonsocial"))
## distinguish between social/nonsocial trials
bigData_social <- bigData[bigData$condition == "social", ]
bigData_nonsocial <- bigData[bigData$condition == "nonsocial", ]
## distinguish between error/correct trials
bigData_err <- bigData_nonsocial[bigData_nonsocial$accuracy == -1, ]
bigData_corr <- bigData_nonsocial[bigData_nonsocial$accuracy == 1, ]
## distinguish between PKU/UCL trials
bigData_PKU <- bigData_nonsocial[bigData_nonsocial$country == "PKU",]
bigData_UCL <- bigData_nonsocial[bigData_nonsocial$country == "UCL",]
## distinguish between both PKU/UCL trials and correct/error trials on non-social condition
bigData_PKU_correct <- bigData_nonsocial[bigData_nonsocial$country == "PKU" & bigData_nonsocial$accuracy == 1,]
bigData_UCL_correct <- bigData_nonsocial[bigData_nonsocial$country == "UCL" & bigData_nonsocial$accuracy == 1,]
bigData_PKU_incorrect <- bigData_nonsocial[bigData_nonsocial$country == "PKU" & bigData_nonsocial$accuracy == -1,]
bigData_UCL_incorrect <- bigData_nonsocial[bigData_nonsocial$country == "UCL" & bigData_nonsocial$accuracy == -1,]
## distinguish between PKU/UCL dataset
PKU = c(demoData[1:length(subjects),])
UCL = c(demoData[length(subjects)+1:nrow(demoData),])
#distinct effects of RT across countries?
confModel_RT = lmer(conf ~ country*(logRT + accuracy + logRT:accuracy) + (1 +  accuracy + logRT|subj), data=bigData_nonsocial
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(confModel_RT)
print(summary(confModel_RT))
print(Anova(confModel_RT, type = 3))
coef(summary(confModel_RT)) #get the contrast statistics
rm(list=ls())
require(R.matlab)
require(lme4)
require(car)
require(optimx)
require(arules) #used to discretize conf adviser on line 94
require(plyr)#used to mapvalues on line 96
bigData = NULL
nat = c('PKU', 'UCL')
j=1
for (d in 1:2) {
if (d == 1) {
dataset = "PKU"
dataDir = "~/Dropbox/Github/CulturalMetacognition/DATA/EXP2/PKU_data/PKU_data/"
filePrefix = "Data_sub_"
suffix = "_2"
subjects = c(403, seq(404,418), seq(420,432), seq(434,435), seq(437,443), seq(445,459))
} else if ( d == 2) {
dataset = "UCL"
dataDir = "~/Dropbox/Github/CulturalMetacognition/DATA/EXP2/UCL_data/UCL_data/"
filePrefix = "Data_sub_"
suffix = "_2"
subjects =c(seq(25,76), 79)
}
# Extract out data from mat files
for (s in 1:length(subjects)){
## EXTRACT DATA FILE
setwd(dataDir)
DATA = readMat(paste(filePrefix,subjects[s],suffix,'.mat',sep=""))
dat = DATA$locDATA
precoh = NULL
postcoh = NULL
RT = NULL
conf = NULL
conf_dir = NULL
confRT = NULL
response = NULL
dir = NULL
conf_adv = NULL
task = NULL
pde_level = NULL
precoh = dat[,,1]$dots.coherence[1,]
postcoh = dat[,,1]$post.coherence[1,]
coh = sort(unique(precoh))
precoh_cat = ifelse(precoh %in% coh[1], -0.5,
ifelse(precoh %in% coh[2], 0, 0.5))
postcoh_cat = ifelse(postcoh %in% coh[1], -0.5,
ifelse(postcoh %in% coh[2], 0, 0.5))
RT = dat[,,1]$reaction.time.button[1,]
conf = dat[,,1]$mouse.response[1,]
conf_dir = dat[,,1]$mouse.response.dir[1,]
confRT = dat[,,1]$reaction.time.mouse[1,]
response = dat[,,1]$button.response[1,] - 1
response[response == 0] = -1
dir = dat[,,1]$dots.direction[1,]/360
dir[dir == 0.5] = -1
accuracy = response == dir
accuracy[accuracy == 0] <- -1
logRT = scale(log(RT))
logConfRT = scale(log(confRT))
task = dat[,,1]$condition[1,]
conf_adv = dat[,,1]$conf.adv[1,]
conf_adv_dir = NA
for(i in 1:length(precoh_cat)){
if(conf_adv[i] < 0.5){
conf_adv_dir[i] <- (1 - conf_adv[i])}
else{
conf_adv_dir[i] <- conf_adv[i]
}
}
conf_adv_dir[conf_adv_dir == 99] <- NA
# get each ID's binned conf
conf_adv_cat = quantile(conf_adv_dir, probs = c(0.33, 0.66), na.rm = T, names = F)
conf_adv_cat = discretize(conf_adv_dir, method = "fixed", breaks = c(0.5, conf_adv_cat[1], conf_adv_cat[2], 1), include.lowest = TRUE, labels = FALSE)
conf_adv_cat <- mapvalues(conf_adv_cat, from = c(1,2,3), to = c(-0.5, 0, 0.5))
conf_inv = NA
for (i in 1:length(precoh_cat)){
if(dir[i] == -1){
conf_inv[i] <- (1 - conf_dir[i])}
else{
conf_inv[i] <- conf_dir[i]
}
}
#get PDE-level dependent on the task condition
for(i in 1:length(postcoh_cat)){
if(task[i] == 1){
pde_level[i] <- (conf_adv_cat[i])}
else{
pde_level[i] <- postcoh_cat[i]
}
}
a_adv = dat[,,1]$a.adv[1,]
a_adv[a_adv == 99] <- NA
a_adv = a_adv-1    # get from 1/2 to 0/1
agree = a_adv == response
agree = agree -0.5#to get -0.5 or 0.5
subj = rep(subjects[s], length(accuracy))
country = rep(d, length(accuracy))
subData1 = data.frame("country" = country, "subj"=subj, "task"=task, "dir"=dir, "precoh"=precoh, "postcoh"=postcoh,"precoh_cat"=precoh_cat, "postcoh_cat"=postcoh_cat, "conf"=conf, "conf_inv" = conf_inv, "conf_adv" = conf_adv, "conf_adv_dir" = conf_adv_dir, "conf_adv_cat" = conf_adv_cat, "conf_dir" = conf_dir, "logConfRT"=logConfRT, "response"=response, "logRT"=logRT, "accuracy"=accuracy, "a_adv" = a_adv, "agree" = agree, "pde_level"= pde_level)
bigData = rbind(bigData, subData1)
j=j+1 # total subject counter
}
}
# Fit regression models
bigData$subj <- factor(bigData$subj)
bigData$country <- factor(bigData$country, levels = c(1,2), labels = c("PKU", "UCL"))
bigData$phase <- factor(bigData$phase, levels = c(1,2), labels = c("first", "second"))
## distinguish between social/nonsocial trials
bigData_social <- bigData[bigData$task == 1, ]
bigData_nonsocial <- bigData[bigData$task == 0, ]
## distinguish between error/correct trials
bigData_err <- bigData_social[bigData_social$accuracy == -1, ]
bigData_corr <- bigData_social[bigData_social$accuracy == 1, ]
## distinguish between PKU/UCL trials
bigData_PKU <- bigData_social[bigData_social$country == "PKU",]
bigData_UCL <- bigData_social[bigData_social$country == "UCL",]
## distinguish between both PKU/UCL trials and correct/error trials on non-social condition
bigData_PKU_correct <- bigData_social[bigData_social$country == "PKU" & bigData_social$accuracy == 1,]
bigData_UCL_correct <- bigData_social[bigData_social$country == "UCL" & bigData_social$accuracy == 1,]
bigData_PKU_incorrect <- bigData_social[bigData_social$country == "PKU" & bigData_social$accuracy == -1,]
bigData_UCL_incorrect <- bigData_social[bigData_social$country == "UCL" & bigData_social$accuracy == -1,]
#Main Model with pre*pos
# distinct effects of RT across countries?
confModel_RT = lmer(conf ~ country*(logRT + accuracy + logRT:accuracy) + (1 +  accuracy + logRT|subj), data=bigData_social
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(confModel_RT)
print(summary(confModel_RT))
print(Anova(confModel_RT, type = 3))
coef(summary(confModel_RT)) #get the contrast statistics
