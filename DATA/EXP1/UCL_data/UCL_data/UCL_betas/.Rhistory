if (acc[t] ==1 & keypress[t] == 87){
dir[t]= -1}
else if (acc[t] == 0 && keypress[t]==69){
dir[t] = -1}
}##correct and chose left, dir == -1 (left) or wrong and chose right, dir == -1 (left)
#get all vars behind each other per subject
subj = rep(s, length(acc))
group = rep(0.5, length(acc))
menta = rep(MCQ_emo[s+40], length(acc))
subData2 = data.frame("subj"=subj,"group"=group, "dir"=dir, "acc"=acc, "conf"=conf, "logRT"=logRT, "menta"=menta)
#add to larger file
CTLData = rbind(CTLData, subData2)
}
bigData <- rbind(asdData, CTLData)
# Factors
bigData$subj <- factor(bigData$subj)
bigData$group <- factor(bigData$group, labels=c("ASD", "comparison"))
# skip rows with NaNs
bigData_clean <- na.omit(bigData)
# get median MCQ_feelings
MCQ_med <- median(bigData_clean$menta)
bigData_clean$menta_bi= cut(as.numeric(bigData_clean$menta),breaks=c(min(bigData_clean$menta),MCQ_med,max(bigData_clean$menta)),include.lowest=T, labels=c("low", "high"))
bigData_clean$menta_bi <- factor(bigData_clean$menta_bi)
## distinguish between error/correct trials
bigData_err <- bigData_clean[bigData_clean$acc == -0.5, ]
bigData_corr <- bigData_clean[bigData_clean$acc == 0.5, ]
asdData_err <- asdData[asdData$acc == -0.5, ]
asdData_corr <- asdData[asdData$acc == 0.5, ]
CTLData_err <- CTLData[CTLData$acc == -0.5, ]
CTLData_corr <- CTLData[CTLData$acc == 0.5, ]
bigData_lmenta <- bigData_clean[bigData_clean$menta_bi == "low", ]
bigData_hmenta <- bigData_clean[bigData_clean$menta_bi == "high", ]
## Get the coefficients for error-correct independent analyses
# correct, ASD
ASD_corr_coef = lmer(conf ~ logRT + (1 + logRT|subj), data=asdData_corr
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(ASD_corr_coef)
fix.se <- sqrt(diag(vcov(ASD_corr_coef)))
betas <- c(fix, fix.se)
write.csv(betas, file = paste(asdDir, 'regression_betas_ASD_corr.csv'))
# correct, CTL
CTL_corr_coef = lmer(conf ~ logRT + (1 +  logRT|subj), data=CTLData_corr
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(CTL_corr_coef)
fix.se <- sqrt(diag(vcov(CTL_corr_coef)))
betas <- c(fix, fix.se)
write.csv(betas, file = paste(asdDir, 'regression_betas_CTL_corr.csv'))
# error, ASD
ASD_err_coef = lmer(conf ~ logRT + (1 + logRT|subj), data=asdData_err
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(ASD_err_coef)
fix.se <- sqrt(diag(vcov(ASD_err_coef)))
betas <- c(fix, fix.se)
write.csv(betas, file = paste(asdDir, 'regression_betas_ASD_err.csv'))
# error, CTL
CTL_err_coef = lmer(conf ~ logRT + (1 + logRT|subj), data=CTLData_err
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(CTL_err_coef)
fix.se <- sqrt(diag(vcov(CTL_err_coef)))
betas <- c(fix, fix.se)
write.csv(betas, file = paste(asdDir, 'regression_betas_CTL_err.csv'))
## Get the coefficients for error-correct independent analyses
# correct, ASD
ASD_corr_coef = lmer(conf ~ logRT + (1 + logRT|subj), data=asdData_corr
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(ASD_corr_coef)
fix.se <- sqrt(diag(vcov(ASD_corr_coef)))
betas <- c(fix, fix.se)
write.csv(betas, file = paste(asdDir, 'regression_betas_ASD_corr.csv'))
# correct, CTL
CTL_corr_coef = lmer(conf ~ logRT + (1 +  logRT|subj), data=CTLData_corr
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(CTL_corr_coef)
fix.se <- sqrt(diag(vcov(CTL_corr_coef)))
betas <- c(fix, fix.se)
write.csv(betas, file = paste(asdDir, 'regression_betas_CTL_corr.csv'))
# error, ASD
ASD_err_coef = lmer(conf ~ logRT + (1 + logRT|subj), data=asdData_err
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(ASD_err_coef)
fix.se <- sqrt(diag(vcov(ASD_err_coef)))
betas <- c(fix, fix.se)
write.csv(betas, file = paste(asdDir, 'regression_betas_ASD_err.csv'))
# error, CTL
CTL_err_coef = lmer(conf ~ logRT + (1 + logRT|subj), data=CTLData_err
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(CTL_err_coef)
fix.se <- sqrt(diag(vcov(CTL_err_coef)))
betas <- c(fix, fix.se)
write.csv(betas, file = paste(asdDir, 'regression_betas_CTL_err.csv'))
## get the coefficients for Figure 3a
# lowMENTA
lmenta_coef = lmer(conf ~ logRT + (1 + logRT|subj), data=bigData_lmenta
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(lmenta_coef)
fix.se <- sqrt(diag(vcov(lmenta_coef)))
betas <- c(fix, fix.se)
write.csv(betas, file = paste(asdDir, 'regression_betas_lmenta.csv'))
# highMENTA
hmenta_coef = lmer(conf ~ logRT + (1 +  logRT|subj), data=bigData_hmenta
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE)))
fix <- fixef(hmenta_coef)
fix.se <- sqrt(diag(vcov(hmenta_coef)))
betas <- c(fix, fix.se)
write.csv(betas, file = paste(asdDir, 'regression_betas_hmenta.csv'))
library(Rmisc) ##for summ stats
require(lme4)
require(car)
library(ggplot2)
library(nlme)
library(plyr)
library(outliers)
library(reshape2)##to rearrange data for plot autho
library(R.matlab)
## set the path & load in data
dataDir <- "~/Dropbox/Populism_PoliticalAttitudes/Online_paper/data/"
data <- read.csv(paste(dataDir,"OnlineData.csv", sep = ""), header=TRUE, sep=",", na.strings = c("#NULL!"))
#Facors
data$id <- factor(data$id)
data$Topic<-factor(data$video_2, levels = c(2,1,3), labels = c("climate change","immigration","healthcare"))
data$Appraisal<-factor(data$video_3, levels = c(3, 2,1,3), labels = c("uncertain", "blame","threat"))
## PART 0: subject characteristics
mean(data$leftright, na.rm = T)
sd(data$leftright, na.rm = T)/sqrt(length(data$leftright))
mean(data$INDEX_CLIMATE, na.rm = T)
sd(data$INDEX_CLIMATE, na.rm = T)/sqrt(length(data$INDEX_CLIMATE))
mean(data$INDEX_MIGRATION, na.rm = T)
sd(data$INDEX_MIGRATION, na.rm = T)/sqrt(length(data$INDEX_MIGRATION))
mean(data$INDEX_HEALTH, na.rm = T)
sd(data$INDEX_HEALTH, na.rm = T)/sqrt(length(data$INDEX_HEALTH))
## Now I standardize the covariates
data$age <- scale(data$lft4)
data$sex <- data$geslacht-1.5
data$edu <- scale(data$opleiding)
data$pol_autho <- scale(data$INDEX_AUTHO)
data$pol_scep <- scale(data$INDEX_CYNICISM)
data$PositionCl <- scale(data$INDEX_CLIMATE)
data$PositionHe <- scale(data$INDEX_HEALTH)
data$PositionImm <- scale(data$INDEX_MIGRATION)
data$PVV_pre <- scale(data$PVV_pre)
data$GL_pre <- scale(data$GL_pre)
data$SP_pre <- scale(data$SP_pre)
data$leftright <- scale(data$leftright)
## Now I standardize the DV
data$FEAR <- scale(data$FEAR)
data$ANGER <- scale(data$ANGER)
data$NEG_AFFECT <- scale(data$NEG_AFFECT)
data$PVV_post <- scale(data$PVV_post)
data$GL_post <- scale(data$GL_post)
data$SP_post <- scale(data$SP_post)
data$importance_gezondheidsz <- scale(data$V13_1)
data$importance_immigr <- scale(data$V13_2)
data$importance_milieu <- scale(data$V13_3)
data$video_agreement <- scale(data$V10_4)
data$video_realiability <-scale(data$V10_5)
data$video_sharing <- scale(data$V10_6)
## PART 1: VIDEO VALIDATION
## Behavioural effects: Party attitdues
party_appraisal <- lm(PVV_post ~ Appraisal + Topic + NEG_AFFECT + Topic * NEG_AFFECT + age + sex + edu + pol_autho + pol_scep+ PositionImm + PositionCl + PositionHe +Topic*NEG_AFFECT, data=data)
party_appraisal <- lm(GL_post ~ Appraisal + Topic + NEG_AFFECT + Topic * NEG_AFFECT + age + sex + edu + pol_autho + pol_scep+ PositionImm + PositionCl + PositionHe + Topic*NEG_AFFECT, data=data)
print(summary(party_appraisal))
print(Anova(party_appraisal, type = 3))
coef(summary(party_appraisal))
##PART TWO: EMOTION EFFECTS
#H1.1
negative_appraisal <- lm(NEG_AFFECT ~ Appraisal + Topic + age + sex + edu + pol_autho + pol_scep + PositionImm + PositionCl + PositionHe + leftright, data=data)
print(summary(negative_appraisal))
print(Anova(negative_appraisal, type = 3))
coef(summary(negative_appraisal))
library(Rmisc) ##for summ stats
require(lme4)
require(car)
library(ggplot2)
library(nlme)
library(plyr)
library(outliers)
library(reshape2)##to rearrange data for plot autho
library(R.matlab)
## set the path & load in data
dataDir <- "~/Dropbox/Populism_PoliticalAttitudes/Online_paper/data/"
data <- read.csv(paste(dataDir,"OnlineData.csv", sep = ""), header=TRUE, sep=",", na.strings = c("#NULL!"))
#Facors
data$id <- factor(data$id)
data$Topic<-factor(data$video_2, levels = c(2,1,3), labels = c("climate change","immigration","healthcare"))
data$Appraisal<-factor(data$video_3, levels = c(3,2,1), labels = c("uncertain", "blame","threat"))
## PART 0: subject characteristics
mean(data$leftright, na.rm = T)
sd(data$leftright, na.rm = T)/sqrt(length(data$leftright))
mean(data$INDEX_CLIMATE, na.rm = T)
sd(data$INDEX_CLIMATE, na.rm = T)/sqrt(length(data$INDEX_CLIMATE))
mean(data$INDEX_MIGRATION, na.rm = T)
sd(data$INDEX_MIGRATION, na.rm = T)/sqrt(length(data$INDEX_MIGRATION))
mean(data$INDEX_HEALTH, na.rm = T)
sd(data$INDEX_HEALTH, na.rm = T)/sqrt(length(data$INDEX_HEALTH))
## Now I standardize the covariates
data$age <- scale(data$lft4)
data$sex <- data$geslacht-1.5
data$edu <- scale(data$opleiding)
data$pol_autho <- scale(data$INDEX_AUTHO)
data$pol_scep <- scale(data$INDEX_CYNICISM)
data$PositionCl <- scale(data$INDEX_CLIMATE)
data$PositionHe <- scale(data$INDEX_HEALTH)
data$PositionImm <- scale(data$INDEX_MIGRATION)
data$PVV_pre <- scale(data$PVV_pre)
data$GL_pre <- scale(data$GL_pre)
data$SP_pre <- scale(data$SP_pre)
data$leftright <- scale(data$leftright)
## Now I standardize the DV
data$FEAR <- scale(data$FEAR)
data$ANGER <- scale(data$ANGER)
data$NEG_AFFECT <- scale(data$NEG_AFFECT)
data$PVV_post <- scale(data$PVV_post)
data$GL_post <- scale(data$GL_post)
data$SP_post <- scale(data$SP_post)
data$importance_gezondheidsz <- scale(data$V13_1)
data$importance_immigr <- scale(data$V13_2)
data$importance_milieu <- scale(data$V13_3)
data$video_agreement <- scale(data$V10_4)
data$video_realiability <-scale(data$V10_5)
data$video_sharing <- scale(data$V10_6)
## PART 1: VIDEO VALIDATION
## Behavioural effects: Party attitdues
party_appraisal <- lm(PVV_post ~ Appraisal + Topic + NEG_AFFECT + Topic * NEG_AFFECT + age + sex + edu + pol_autho + pol_scep+ PositionImm + PositionCl + PositionHe +Topic*NEG_AFFECT, data=data)
party_appraisal <- lm(GL_post ~ Appraisal + Topic + NEG_AFFECT + Topic * NEG_AFFECT + age + sex + edu + pol_autho + pol_scep+ PositionImm + PositionCl + PositionHe + Topic*NEG_AFFECT, data=data)
print(summary(party_appraisal))
print(Anova(party_appraisal, type = 3))
coef(summary(party_appraisal))
##PART TWO: EMOTION EFFECTS
#H1.1
negative_appraisal <- lm(NEG_AFFECT ~ Appraisal + Topic + age + sex + edu + pol_autho + pol_scep + PositionImm + PositionCl + PositionHe + leftright, data=data)
print(summary(negative_appraisal))
print(Anova(negative_appraisal, type = 3))
coef(summary(negative_appraisal))
fear_appraisal <- lm(FEAR ~ Appraisal + Topic + age + sex + edu + pol_autho + pol_scep + PositionImm + PositionCl + PositionHe + leftright,data=data)
print(summary(fear_appraisal))
print(Anova(fear_appraisal, type = 3))
coef(summary(fear_appraisal))
#H1.3
anger_appraisal <- lm(ANGER ~ Appraisal + Topic + age + sex + edu + pol_autho + pol_scep + PositionImm + PositionCl + PositionHe + leftright, data=data)
print(summary(anger_appraisal))
print(Anova(anger_appraisal, type = 3))
coef(summary(anger_appraisal))
#H1: Agreement
agr_appraisal <- lm(video_agreement ~ Appraisal + NEG_AFFECT + Topic + age + sex + edu + pol_autho + pol_scep + leftright + PositionImm + PositionCl + PositionHe + pol_autho*Appraisal + NEG_AFFECT*Appraisal, data=data)
print(summary(agr_appraisal))
print(Anova(agr_appraisal, type = 3))
coef(summary(agr_appraisal))
## EVDP 2019 elisa.plas.18@ucl.ac.uk
# This script replicates the Results section of Experiment 2 in chronological order
rm(list=ls())
require(R.matlab)
require(lme4)
require(car)
require(optimx)
bigData1 = NULL
bigData2 = NULL
demoData = NULL
nat = c('PKU', 'UCL')
j=1
for (d in 1:2) {
if (d == 1) {
dataDir = "~/Dropbox/Github/CulturalMetacognition/DATA/EXP1/PKU_data/PKU_data/"
filePrefix = "Data_sub_"
suffix = "_2"
subjects =c(seq(101,109), seq(111,115), seq(117,141))
} else if (d == 2) {
dataDir = "~/Dropbox/Github/CulturalMetacognition/DATA/EXP1/UCL_data/UCL_data/"
filePrefix = "Data_sub_"
suffix = "_2"
subjects = c(seq(201,204), seq(206, 227), seq(229, 234), seq(236,242))
}
# Extract out data from mat files
for (s in 1:length(subjects)){
setwd(dataDir)
DATA = readMat(paste(filePrefix,subjects[s],suffix,'.mat',sep=""))
dat = DATA$locDATA
precoh = NULL
postcoh = NULL
RT = NULL
conf = NULL
confRT = NULL
response = NULL
dir = NULL
precoh = dat[,,1]$dots.coherence[1,]
postcoh = dat[,,1]$post.coherence[1,]
coh = sort(unique(precoh))
precoh_cat = ifelse(precoh %in% coh[1], -0.5,
ifelse(precoh %in% coh[2], 0, 0.5))
postcoh_cat = ifelse(postcoh %in% coh[1], -0.5,
ifelse(postcoh %in% coh[2], 0, 0.5))
RT = dat[,,1]$reaction.time.button[1,]
conf = dat[,,1]$mouse.response[1,]
confRT = dat[,,1]$reaction.time.mouse[1,]
response = dat[,,1]$button.response[1,] - 1
response[response == 0] = -1
dir = dat[,,1]$dots.direction[1,]/360
dir[dir == 0.5] = -1
accuracy = response == dir
accuracy_dv = accuracy #accuracy when used as dependent variable: 0/1
accuracy[accuracy == 0] <- -1 #accuracy when used as predictor variable: -1/1
logRT = scale(log(RT))
logConfRT = scale(log(confRT))
subj = rep(j, length(accuracy))
country = rep(d, length(accuracy))
experiment = rep(1, length(accuracy))
subData1 = data.frame("country"=country, "subj"=subj, "dir"=dir, "precoh"=precoh, "postcoh"=postcoh, "precoh_cat"=precoh_cat, "postcoh_cat"=postcoh_cat,"conf"=conf, "logConfRT"=logConfRT, "response"=response, "dir"=dir, "logRT"=logRT, "accuracy"=accuracy,"exp" = experiment)
#add to larger file
bigData1 = rbind(bigData1, subData1)
j=j+1 # total subject counter
}
}
j=1
for (d in 1:2) {
if (d == 1) {
dataset = "PKU"
dataDir = "~/Dropbox/Github/CulturalMetacognition/DATA/EXP2/PKU_data/PKU_data/"
filePrefix = "Data_sub_"
suffix = "_2"
subjects = c(403, seq(404,418), seq(420,432), seq(434,435), seq(437,443), seq(445,459))
} else if ( d == 2) {
dataset = "UCL"
dataDir = "~/Dropbox/Github/CulturalMetacognition/DATA/EXP2/UCL_data/UCL_data/"
filePrefix = "Data_sub_"
suffix = "_2"
subjects =c(seq(25,76), 79)
}
# Extract out data from mat files
for (s in 1:length(subjects)){
## EXTRACT DATA FILE
setwd(dataDir)
DATA = readMat(paste(filePrefix,subjects[s],suffix,'.mat',sep=""))
dat = DATA$locDATA
precoh = NULL
postcoh = NULL
RT = NULL
conf = NULL
confRT = NULL
response = NULL
dir = NULL
precoh = dat[,,1]$dots.coherence[1,]
postcoh = dat[,,1]$post.coherence[1,]
coh = sort(unique(precoh))
precoh_cat = ifelse(precoh %in% coh[1], -0.5,
ifelse(precoh %in% coh[2], 0, 0.5))
postcoh_cat = ifelse(postcoh %in% coh[1], -0.5,
ifelse(postcoh %in% coh[2], 0, 0.5))
RT = dat[,,1]$reaction.time.button[1,]
conf = dat[,,1]$mouse.response[1,]
confRT = dat[,,1]$reaction.time.mouse[1,]
response = dat[,,1]$button.response[1,] - 1
response[response == 0] = -1
dir = dat[,,1]$dots.direction[1,]/360
dir[dir == 0.5] = -1
accuracy = response == dir
accuracy[accuracy == 0] <- -1
condition = dat[,,1]$condition[1,]
logRT = scale(log(RT))
logConfRT = scale(log(confRT))
subj = rep(j, length(accuracy))
country = rep(d, length(accuracy))
experiment = rep(2, length(accuracy))
subData1 = data.frame("country"=country, "subj"=subj, "dir"=dir, "precoh"=precoh, "postcoh"=postcoh, "precoh_cat"=precoh_cat, "postcoh_cat"=postcoh_cat,"conf"=conf, "logConfRT"=logConfRT, "response"=response, "dir"=dir, "logRT"=logRT, "accuracy"=accuracy,"exp" = experiment)
#add to larger file
bigData2 = rbind(bigData2, subData1)
j=j+1 # total subject counter
}
}
bigData <- rbind(bigData1, bigData2)
# Factors
bigData$subj <- factor(bigData$subj)
bigData$country <- factor(bigData$country, levels = c(1,2), labels = c("PKU","UCL"))
bigData$exp <- factor(bigData$exp, levels = c(1,2), labels = c("exp1","exp2"))
## SECOND-ORDER PERFORMANCE
confModel_wcountry = lmer(conf ~ exp*(country*(accuracy + precoh_cat + postcoh_cat + precoh_cat:postcoh_cat + precoh_cat:accuracy + postcoh_cat:accuracy + precoh_cat:postcoh_cat:accuracy + logRT)) + (1 + accuracy + precoh_cat + postcoh_cat + precoh_cat:postcoh_cat + precoh_cat:accuracy + postcoh_cat:accuracy + precoh_cat:postcoh_cat:accuracy + logRT|subj), data=bigData
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE, REML = FALSE)))
fix <- fixef(confModel_wcountry)
print(summary(confModel_wcountry))
print(Anova(confModel_wcountry, type = 3))
coef(summary(confModel_wcountry)) #get the contrast statistics
fix.se <- sqrt(diag(vcov(confModel_wcountry)))
1
## EVDP 2019 elisa.plas.18@ucl.ac.uk
# This code replicates the Results section in chronological order
rm(list=ls())
require(R.matlab)
require(lme4)
require(car)
require(optimx)
require(pwr)
options(contrasts = c("contr.treatment", "contr.poly")) # This is R defaults but set it anyway to be safe
bigData = NULL
demoData = NULL
nat = c('PKU', 'UCL')
j=1
for (d in 1:2) {
if (d == 1) {
dataDir = "~/Dropbox/Github/CulturalMetacognition/DATA/EXP1/PKU_data/PKU_data/"
filePrefix = "Data_sub_"
suffix = "_2"
subjects =c(seq(101,109), seq(111,115), seq(117,141))
} else if (d == 2) {
dataDir = "~/Dropbox/Github/CulturalMetacognition/DATA/EXP1/UCL_data/UCL_data/"
filePrefix = "Data_sub_"
suffix = "_2"
subjects = c(seq(201,204), seq(206, 227), seq(229, 234), seq(236,242))
}
# Extract out data from mat files
for (s in 1:length(subjects)){
setwd(dataDir)
DATA = readMat(paste(filePrefix,subjects[s],suffix,'.mat',sep=""))
dat = DATA$locDATA
precoh = NULL
postcoh = NULL
RT = NULL
conf = NULL
confRT = NULL
response = NULL
dir = NULL
precoh = dat[,,1]$dots.coherence[1,]
postcoh = dat[,,1]$post.coherence[1,]
coh = sort(unique(precoh))
precoh_cat = ifelse(precoh %in% coh[1], -0.5,
ifelse(precoh %in% coh[2], 0, 0.5))
postcoh_cat = ifelse(postcoh %in% coh[1], -0.5,
ifelse(postcoh %in% coh[2], 0, 0.5))
RT = dat[,,1]$reaction.time.button[1,]
conf = dat[,,1]$mouse.response[1,]
confRT = dat[,,1]$reaction.time.mouse[1,]
response = dat[,,1]$button.response[1,] - 1
response[response == 0] = -1
dir = dat[,,1]$dots.direction[1,]/360
dir[dir == 0.5] = -1
accuracy = response == dir
accuracy_dv = accuracy #accuracy when used as dependent variable: 0/1
accuracy[accuracy == 0] <- -1 #accuracy when used as predictor variable: -1/1
logRT = scale(log(RT))
logConfRT = scale(log(confRT))
subj = rep(j, length(accuracy))
country = rep(d, length(accuracy))
subData1 = data.frame("country"=country, "subj"=subj, "dir"=dir, "precoh"=precoh, "postcoh"=postcoh, "precoh_cat"=precoh_cat, "postcoh_cat"=postcoh_cat,"conf"=conf, "logConfRT"=logConfRT, "response"=response, "dir"=dir, "logRT"=logRT, "accuracy"=accuracy, "accuracy_dv" = accuracy_dv)
#add to larger file
bigData = rbind(bigData, subData1)
j=j+1 # total subject counter
}
setwd(paste(dataDir,nat[d], '_betas', sep = ""))
demoData1<- read.csv(paste(nat[d], '_subject_log.csv',sep=""), header = T, sep = ",", na.strings = "EMPTY")
demoData = rbind(demoData, demoData1)
}
# Factors
bigData$subj <- factor(bigData$subj)
bigData$country <- factor(bigData$country, levels = c(1,2), labels = c("PKU","UCL"))
## distinguish between correct/incorrect & PKU/UCL trials
bigData_correct <- bigData[bigData$accuracy == 1, ]
bigData_incorrect <- bigData[bigData$accuracy == -1, ]
bigData_PKU <- bigData[bigData$country == "PKU",]
bigData_UCL <- bigData[bigData$country == "UCL",]
bigData_PKU_correct <- bigData_correct[bigData_correct$country == "PKU",]
bigData_UCL_correct <- bigData_correct[bigData_correct$country == "UCL",]
bigData_PKU_incorrect <- bigData_incorrect[bigData_incorrect$country == "PKU",]
bigData_UCL_incorrect <- bigData_incorrect[bigData_incorrect$country == "UCL",]
## Demographics described at start of Results section (for Supplementary Table 1, see Github_SuppMat/1.1.Demo.R)
PKU = c(demoData[1:length(subjects),])
UCL = c(demoData[length(subjects)+1:nrow(demoData),])
#age
mean(PKU$age, na.rm = T)
sd(PKU$age, na.rm = T)/sqrt(length(subjects))
mean(UCL$age, na.rm = T)
sd(UCL$age, na.rm = T)/sqrt(length(subjects))
t.test(PKU$age, UCL$age, var.equal = T)
#gender
mean(PKU$gender, na.rm = T)
mean(UCL$gender, na.rm = T)
t.test(PKU$gender, UCL$gender, var.equal = T)
#income: remove outlier in UCL
UCL$income[UCL$income == 3000000] <- NA
#relative to PPP difference UK/China
UCL$income = UCL$income/1.71
mean(PKU$income, na.rm = T)
sd(PKU$income, na.rm = T)/sqrt(length(subjects))
mean(UCL$income, na.rm = T)
sd(UCL$income, na.rm = T)/sqrt(length(subjects))
t.test(PKU$income, UCL$income, var.equal = T)
#IQ
mean(PKU$IQ, na.rm = T)
sd(PKU$IQ, na.rm = T)/sqrt(length(subjects))
mean(UCL$IQ, na.rm = T)
sd(UCL$IQ, na.rm = T)/sqrt(length(subjects))
t.test(PKU$IQ, UCL$IQ, var.equal = T)
## FIRST-ORDER PERFORMANCE
accModel = glmer(accuracy_dv ~ country*precoh_cat + (1 + precoh_cat | subj), data=bigData, family="binomial")
print(summary(accModel))
print(Anova(accModel, type =3))
## RT effects
confModel_RT= lmer(conf ~ country + logRT + country * logRT + (1 + country + logRT|subj), data=bigData
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE, REML = FALSE)))
fix <- fixef(confModel_RT)
print(summary(confModel_RT))
print(Anova(confModel_RT, type = 3))
coef(summary(confModel_RT)) #get the contrast statistics
## RT effects
confModel_RT= lmer(conf ~ country* (logRT + accuracy)  + (1 + accuracy + logRT|subj), data=bigData
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE, REML = FALSE)))
fix <- fixef(confModel_RT)
print(summary(confModel_RT))
print(Anova(confModel_RT, type = 3))
coef(summary(confModel_RT)) #get the contrast statistics
confModel_RT= lmer(conf ~ country* (logRT + accuracy + logRT*accuracy)  + (1 + accuracy + logRT|subj), data=bigData
, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE, optCtrl = list(method = "bobyqa", starttests = FALSE, kkt = FALSE, REML = FALSE)))
fix <- fixef(confModel_RT)
print(summary(confModel_RT))
print(Anova(confModel_RT, type = 3))
coef(summary(confModel_RT)) #get the contrast statistics
